{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Ensemble methods**","metadata":{}},{"cell_type":"markdown","source":"# Iris Dataset\n\nIl dataset Iris è un classico dataset nell'apprendimento automatico e nella statistica, introdotto da Ronald Fisher nel 1936. È comunemente utilizzato per attività di classificazione e clustering.\n\n## Caratteristiche e Struttura\n- **Campioni**: 150 campioni di fiori iris.\n- **Features**:\n  - Lunghezza del sepalo (cm)\n  - Larghezza del sepalo (cm)\n  - Lunghezza del petalo (cm)\n  - Larghezza del petalo (cm)\n- **Classi (Etichette Target)**:\n  - *Iris-setosa*\n  - *Iris-versicolor*\n  - *Iris-virginica*\n\nOgni classe è rappresentata da 50 campioni.\n\n## Caratteristiche Principali\n- **Balanced Dataset**: Ogni classe contiene lo stesso numero di campioni.\n- **Perfect for Beginners**: a sua semplicità e struttura ben definita lo rendono perfetto per scopi didattici.\n- **Separable Classes**:\n  - *Iris-setosa* è linearmente separabile dalle altre due classi.\n  - *Iris-versicolor* e *Iris-virginica* sono più difficili da separare tra loro.\n\n\n# Iris Dataset Classes\n\n<table>\n    <tr>\n        <th>Iris Setosa</th>\n        <th>Iris Versicolor</th>\n        <th>Iris Virginica</th>\n    </tr>\n    <tr>\n        <td><img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSFn-u9Lagrv8pV4zJ8Z1cEqXNL_uo39CrL6A&s\" alt=\"Iris setosa\" width=\"300\" height=\"300\"></td>\n        <td><img src=\"https://encrypted-tbn3.gstatic.com/images?q=tbn:ANd9GcSJqxUtJiLfMX5aIoyPTPz7rMdjxgWagMlBzt0QbfATKzRqH4XnMMDN5aBrU1FvRt19jkHMOrIefjywQlDg9rOeKC6JbA72Wf--jqHD-g\" alt=\"Iris versicolor\" alt=\"Iris versicolor\" width=\"300\" height=\"300\"></td>\n        <td><img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSQbTwTLA7_7SeTE3B1QOKw0TlB8Rp6NU7vyg&s\" alt=\"Iris virginica\" width=\"300\" height=\"300\"></td>\n    </tr>\n</table>\n\n","metadata":{}},{"cell_type":"markdown","source":"# `plot_decision_boundary` Function\n\nLa funzione `plot_decision_boundary` permette di visualizzare i margini decisionali di un classificatore. In questo modo è possibile visualizzare come il modello distingue le feature di campioni assegnati a classi diverse.\n\n1. **Parametri**:\n    - `clf`: Il classificatore allenato che ha il metodo `predict`.\n    - `X`: La matrice dei dati di input, per cui si assume una dimensione 2D per la visualizzazione.\n    - `y`: Le labels corrispondenti ai dati `X`.\n\n2. **Output**:\n    - Un grafico 2D contenente:\n        - **Features**: `Feature 1` e `Feature 2` lungo l' asse x e y (se è stata usata PCA, queste sono le 2 componenti).\n        - **Regioni decisionali**: Colori diversi indicano regioni classificate come labels diverse.\n        - **Camponi**: I punti del dataset (`X`) sono sovrapposti alle regioni e colorati in base alla loro label corretta (`y`).\n\n### **Sintassi**\n\n```python\nplot_decision_boundary(trained_model, X_data, y_data)\n```\n","metadata":{}},{"cell_type":"code","source":"# Helper function to create the plot\n# and visualize the decision boundary\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef plot_decision_boundary(clf, X, y):\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500), np.linspace(y_min, y_max, 500))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.figure(figsize=(10, 6))\n    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', cmap=plt.cm.RdYlBu)\n    plt.title(\"Decision Boundary Visualization\")\n    plt.xlabel(\"Feature 1\")\n    plt.ylabel(\"Feature 2\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T13:40:31.445254Z","iopub.execute_input":"2025-05-15T13:40:31.445631Z","iopub.status.idle":"2025-05-15T13:40:31.453981Z","shell.execute_reply.started":"2025-05-15T13:40:31.445604Z","shell.execute_reply":"2025-05-15T13:40:31.452978Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Funzione `train_test_split()` :\n\n---\n\n## **train_test_split()**\n\nLa funzione `train_test_split()` è parte del modulo `sklearn.model_selection`. Viene utilizzata per dividere un dataset in training e test set.\n\n\n### **Esempio**:\n```python\nfrom sklearn.model_selection import train_test_split\n\n# Splitting the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Training data: {len(X_train)} samples\")\nprint(f\"Testing data: {len(X_test)} samples\")\n```\n","metadata":{}},{"cell_type":"markdown","source":"# **Esercizio 1: Classifichiamo il dataset Iris con un DecisionTree**\n\nEseguire tutti gli step di preparazione per l' allenamento di un DecisionTree. Per lo split dai dati in training e test utilizzare:\n\n- `test_size` = `0.3`\n\n- `random_state` = `42`","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# svolgimento...\n# Split 70/30\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Decision Tree\ndt_clf = DecisionTreeClassifier(random_state=42)\ndt_clf.fit(X_train, y_train)\ny_pred = dt_clf.predict(X_test)\n\n# Valutazione\nprint(\"Accuracy Decision Tree:\", accuracy_score(y_test, y_pred))\n\n# Visualizzazione\nplt.figure(figsize=(12, 6))\nplot_tree(dt_clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)\nplt.title(\"Decision Tree on Iris\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T13:40:31.457387Z","iopub.execute_input":"2025-05-15T13:40:31.457731Z","iopub.status.idle":"2025-05-15T13:40:32.139130Z","shell.execute_reply.started":"2025-05-15T13:40:31.457700Z","shell.execute_reply":"2025-05-15T13:40:32.138206Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Eesercizio 2: Implementare ensamble methods**\n\nUna volta allenato il DecisionTree nell' esercizio 1, vogliamo applicarvi i metodi di ensamble. Nello specifico andremo a implementare:\n\n* **AdaBoost**\n* **Bagging**\n* **Random Forest**\n\nDi seguito vediamo la sintassi di ognuno di questi.","metadata":{}},{"cell_type":"markdown","source":"## 1. **AdaBoostClassifier**: \n\nL' `AdaBoostClassifier` crea un insieme di alberi decisionali deboli. Assegna un peso a ciascun albero e li combina per formare un modello più robusto.\n\n### Example:\n```python\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Train the model\nada_clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=50, random_state=42)\nada_clf.fit(X_train, y_train)\n\n# Get predictions\npredictions = ada_clf.predict(X_test)\n\n```\n\n---\n\n## 2. **RandomForestClassifier**: \n\nIl `RandomForestClassifier` costruisce più alberi in parallelo e combina i loro risultati per migliorare l'accuratezza.\n\n### Example:\n```python\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Train the model\nrf_clf = RandomForestClassifier(n_estimators=10, max_depth=3, random_state=42)\nrf_clf.fit(X_train, y_train)\n\n# Get predictions\npredictions = rf_clf.predict(X_test)\n\n```\n\n#### **N.B. RandomForest non richiede come argomento il classificatore, differentemente dagli altri metodi.**\n\n---\n\n## 3. **BaggingClassifier**:\n\nIl `BaggingClassifier` combina molteplici modelli base (come DecisionTree) utilizzando la tecnica del bootstrapping per ridurre la varianza.\n\n### Example:\n```python\nfrom sklearn.ensemble import BaggingClassifier\n\n# Train the model\nbag_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\nbag_clf.fit(X_train, y_train)\n\n# Get predictions\npredictions = bag_clf.predict(X_test)\n\n```\n","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier\n\n# Creare gli oggetti ensemble\n\n# svolgimento...\n# AdaBoost con alberi deboli (max_depth=1)\nada_model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=50, random_state=42)\n\n# Bagging con alberi completi\nbagging_model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50,random_state=42)\n\n# Random Forest con 100 alberi\nrf_model = RandomForestClassifier(n_estimators=100,random_state=42)\n\n# Allenare i modelli\n# svolgimento...\n# AdaBoost\nada_model.fit(X_train, y_train)\n\n# Bagging\nbagging_model.fit(X_train, y_train)\n\n# Random Forest\nrf_model.fit(X_train, y_train)\n\n\n# Valutare e stampare le prestazioni dei modelli\n\n# svolgimento...\ny_pred_ada = ada_model.predict(X_test)\ny_pred_bagging = bagging_model.predict(X_test)\ny_pred_rf = rf_model.predict(X_test)\n\n# Accuratezze\nacc_ada = accuracy_score(y_test, y_pred_ada)\nacc_bagging = accuracy_score(y_test, y_pred_bagging)\nacc_rf = accuracy_score(y_test, y_pred_rf)\n\n# Stampa dei risultati\nprint(f\"Accuratezza AdaBoost      : {acc_ada:.2f}\")\nprint(f\"Accuratezza Bagging       : {acc_bagging:.2f}\")\nprint(f\"Accuratezza Random Forest : {acc_rf:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T13:40:32.140619Z","iopub.execute_input":"2025-05-15T13:40:32.140894Z","iopub.status.idle":"2025-05-15T13:40:32.450872Z","shell.execute_reply.started":"2025-05-15T13:40:32.140874Z","shell.execute_reply":"2025-05-15T13:40:32.449971Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Funzioni utili per Ensemble Models\n\n---\n\n## **1. Hard Voting tra tre classificatori**\n\n### **Descrizione**:\nCombina le predizioni da tre classificatori selezionando la casse più votata per ogni campione.\n\n### **Parametri**:\n- `pred1` (numpy array): Predizioni del classificatore 1.\n- `pred2` (numpy array): Predizioni del classificatore 2.\n- `pred3` (numpy array): Predizioni del classificatore 3.\n\n### **Output**:\n- Restituisce un numpy array conenente la classe più votata per ogni campione.\n\n### **Sintassi**:\n```python\nvoted = hard_voting(pred1_test, pred2_test, pred3_test)\nprint(\"Hard Voting Predictions:\", voted)\n```\n\n---\n\n## **2. Allineare predizioni a più classi**\n\n### **Descrizione**:\nAllinea le probabilità in modo da rendere compatibili diversi subsets quando vengono combinati.\n\n### **Parametri**:\n- `pred` (numpy array): Probabilità predette da un classificatore.\n- `classes_present` (list): Classi conosciute dal classificatore.\n- `n_classes` (int, optional): Numero totale di classe, default è 3.\n\n### **Output**:\n- Ritorna un numpy array con le proabilità allineate tra tutte le classi\n- Returns a numpy array with probabilities aligned across all classes.\n\n### **Sintassi**:\n```python\naligned_probs = align_predictions(pred, [0, 1], n_classes=3)\nprint(\"Aligned Probabilities:\", aligned_probs)\n\nMatrice originale [[0.7, 0.3], [0.4, 0.6]]\ndiventa\nMatrice allineata [[0.7, 0, 0.3], [0.4, 0, 0.6]]\n\n```\n\n---\n\n## **3. Plot Decision Boundary**\n\n### **Descrizione**:\nVisualizza i margini decisionali per vari ensamble methods, inclusi `expert1`, `expert2`, `expert3`, `base`, `soft_voting`, `hard_voting`, e `gating`.\n\n### **Parametri**:\n- `X` (numpy array): Dati per la visualizzazione (2D).\n- `y` (numpy array): Lables originali.\n- `clf1`, `clf2`, `clf3` (classifiers, optional): Esperti usati per calcolare prediction.\n- `base` (classifier, optional): Classificatore base.\n- `mode` (string): Metodo di ensamble per la visualizzazione. I valori possibili sono: `\"expert1\"`, `\"expert2\"`, `\"expert3\"`, `\"base\"`, `\"soft_voting\"`, `\"hard_voting\"`, `\"gating\"`.\n\n\n### **Output**:\n- Displays a decision boundary plot for the selected mode.\n\n### **Usage**:\n```python\nplot_decision_boundary(x_test, y_test, expert1=expert1, clf2=expert2, clf3=expert3, base=base_network, mode=\"soft_voting\")\n```\n","metadata":{}},{"cell_type":"markdown","source":"# **Esercizio 3: Implementare Mixture of Experts**\n\nNel seguente esercizio vogliamo implementare un meccanismo di Mixture of Experts. Poichè in iris sono presenti 3 classi, vogliamo allenare 3 classificatori (cioè 3 esperi), rispettivamente:\n\n- esperto 1: riconosce tra la classe 0 e la classe 1.\n- esperto 2: riconosce tra la classe 0 e la classe 2.\n- esperto 3: riconosce tra la classe 1 e la classe 2.\n\nInfine utilizzare la funzione `plot_modes` per plottare le diverse modalità.","metadata":{}},{"cell_type":"code","source":"def hard_voting(pred1, pred2, pred3):\n    combined = np.vstack([pred1, pred2, pred3]).T\n    voted = []\n    for sample in combined:\n        counts = np.bincount(sample)\n        most_common_label = counts.argmax()\n        voted.append(most_common_label)\n    return np.array(voted)\n\ndef align_predictions(pred, classes_present, n_classes=3):\n    aligned = np.zeros((len(pred), n_classes))\n    for idx, class_label in enumerate(classes_present):\n        aligned[:, class_label] = pred[:, idx]\n    return aligned\n\ndef plot_modes(X, y, clf1=None, clf2=None, clf3=None, base=None, mode=\"expert1\"):\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500),\n                         np.linspace(y_min, y_max, 500))\n    grid = np.c_[xx.ravel(), yy.ravel()]\n\n    if mode.startswith(\"expert\"):\n        expert_map = {\n            \"expert1\": (clf1, [0, 1]),\n            \"expert2\": (clf2, [0, 2]),\n            \"expert3\": (clf3, [1, 2]),\n        }\n        clf, classes = expert_map[mode]\n        Z = clf.predict(grid).reshape(xx.shape)\n        title = f\"Decision Boundary - {mode.capitalize()}\"\n\n    elif mode == \"base\":\n        Z = base.predict(grid).reshape(xx.shape)\n        title = \"Decision Boundary - Base Network\"\n\n    elif mode == \"soft_voting\":\n        Z1 = align_predictions(clf1.predict_proba(grid), [0, 1])\n        Z2 = align_predictions(clf2.predict_proba(grid), [0, 2])\n        Z3 = align_predictions(clf3.predict_proba(grid), [1, 2])\n        Z = (Z1 + Z2 + Z3).argmax(axis=1).reshape(xx.shape)\n        title = \"Decision Boundary - Soft Voting\"\n\n    elif mode == \"hard_voting\":\n        Z1 = clf1.predict(grid)\n        Z2 = clf2.predict(grid)\n        Z3 = clf3.predict(grid)\n        Z = np.array([Z1, Z2, Z3])\n        Z = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=Z)\n        Z = Z.reshape(xx.shape)\n        title = \"Decision Boundary - Hard Voting\"\n\n    elif mode == \"gating\":\n        gating_weights = base.predict_proba(grid)\n        gating_weights /= gating_weights.sum(axis=1, keepdims=True)\n        Z1 = align_predictions(clf1.predict_proba(grid), [0, 1])\n        Z2 = align_predictions(clf2.predict_proba(grid), [0, 2])\n        Z3 = align_predictions(clf3.predict_proba(grid), [1, 2])\n        Z = gating_weights * Z1 + gating_weights * Z2 + gating_weights * Z3\n        Z = Z.argmax(axis=1).reshape(xx.shape)\n        title = \"Decision Boundary - Mixture of Experts (Gating)\"\n\n    else:\n        raise ValueError(f\"Unknown mode: {mode}\")\n\n    plt.figure(figsize=(10, 6))\n    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', cmap=plt.cm.RdYlBu)\n    plt.title(title)\n    plt.xlabel(\"Feature 1\")\n    plt.ylabel(\"Feature 2\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T13:40:32.451963Z","iopub.execute_input":"2025-05-15T13:40:32.452273Z","iopub.status.idle":"2025-05-15T13:40:32.469370Z","shell.execute_reply.started":"2025-05-15T13:40:32.452246Z","shell.execute_reply":"2025-05-15T13:40:32.468411Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from matplotlib.colors import ListedColormap\n# Caricare il dataset Iris\n\n# svolgimento...\niris = load_iris()\nX = iris.data\ny = iris.target\n\n\n# Split del dataset in training e test set\n\n# svolgimento...\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\n# Applicare lo scaling e PCA\n\n# svolgimento...\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\npca = PCA(n_components=2)\nX_train_pca = pca.fit_transform(X_train_scaled)\nX_test_pca = pca.transform(X_test_scaled)\n\n# Creare 3 modelli di Decision Tree, ognuno dovrà essere esperto in una coppia di classi\n# N.B. ogni esperto dovrà essere allenato su un sottoinsieme contentente solo \n# le classi di competenza\n\n# svolgimento...\ndef get_binary_subset(X, y, class1, class2):\n    mask = np.isin(y, [class1, class2])\n    X_bin = X[mask]\n    y_bin = y[mask]\n    # Rietichettatura: la classe class1 -> 0, class2 -> 1\n    y_bin = (y_bin == class2).astype(int)\n    return X_bin, y_bin\n\n# Expert 1: classi 0 vs 1\nX1, y1 = get_binary_subset(X_train_pca, y_train, 0, 1)\nexpert1 = DecisionTreeClassifier(random_state=0).fit(X1, y1)\n\n# Expert 2: classi 1 vs 2\nX2, y2 = get_binary_subset(X_train_pca, y_train, 1, 2)\nexpert2 = DecisionTreeClassifier(random_state=0).fit(X2, y2)\n\n# Expert 3: classi 0 vs 2\nX3, y3 = get_binary_subset(X_train_pca, y_train, 0, 2)\nexpert3 = DecisionTreeClassifier(random_state=0).fit(X3, y3)\n\n\n# Creare un modello di Decision Tree che funge da base network\n# N.B. il base network dovrà essere allenato su tutte le classi, sarà la nostra gating network\n\n# svolgimento...\nbase = DecisionTreeClassifier(random_state=0)\nbase.fit(X_train_pca, y_train) \n\n\n# Estrarre le predizione di ogni esperto sul test set.\n# N.B. Vogliamo le probabilità di appartenenza a ciascuna classe, quindi useremo `predict_proba`. Inoltre \n# ogni esperto riporterà solo le classi di competenza, quindi dovremo allineare le predizioni.\n\n# svolgimento...\ndef get_aligned_proba(clf, X, class1, class2):\n    proba = clf.predict_proba(X)\n    aligned = np.zeros((len(X), 3))\n    aligned[:, class1] = proba[:, 0]\n    aligned[:, class2] = proba[:, 1]\n    return aligned\n\nproba1 = get_aligned_proba(expert1, X_test_pca, 0, 1)\nproba2 = get_aligned_proba(expert2, X_test_pca, 1, 2)\nproba3 = get_aligned_proba(expert3, X_test_pca, 0, 2)\n\n# Estrarre le predizioni del base network sul test set\n\n# svolgimento...\nproba_base = base.predict_proba(X_test_pca)\n\n# Usiamo hard voting per combinare le predizioni degli esperti. \n# N.B. hard voting significa che ogni esperto vota per la sua classe di competenza e il voto più comune\n# vince.\n\n# svolgimento...\npred1 = np.argmax(proba1, axis=1)\npred2 = np.argmax(proba2, axis=1)\npred3 = np.argmax(proba3, axis=1)\n\n# Voto maggioritario\nvotes = np.stack([pred1, pred2, pred3], axis=1)\nhard_voting_pred = np.array([np.bincount(vote_row).argmax() for vote_row in votes])\n\n# Usiamo soft voting per combinare le predizioni degli esperti.\n# N.B. soft voting significa che il voto di ogni esperto ha lo stesso peso.\n\n# svolgimento...\navg_proba = (proba1 + proba2 + proba3) / 3\nsoft_voting_pred = np.argmax(avg_proba, axis=1)\n\n# Usiamo gating network per combinare le predizioni degli esperti.\n# N.B. la gating network calcola le probabilità di appartenenza a ciascuna classe e le usa per pesare\n# le predizioni degli esperti.\n\n# svolgimento...\ngating_weights = proba_base\nweighted_sum = (proba1 * gating_weights[:, [0]] + proba2 * gating_weights[:, [1]] + proba3 * gating_weights[:, [2]])\ngating_pred = np.argmax(weighted_sum, axis=1)\n\nmodes = [\"expert1\", \"expert2\", \"expert3\", \"base\", \"soft_voting\", \"hard_voting\", \"gating\"]\n\n# Plottare le decision boundaries per ogni modalità.\n\n# svolgimento...\ndef get_decision_function(predict_function):\n    def predict_grid(X):\n        return predict_function(X)\n    return predict_grid\n\n# Funzione per plottare decision boundary da predizioni\ndef plot_boundary_from_preds(predict_func, X, y, title, ax):\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n                         np.linspace(y_min, y_max, 200))\n    grid = np.c_[xx.ravel(), yy.ravel()]\n    Z = predict_func(grid)\n    Z = Z.reshape(xx.shape)\n    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n    ax.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.6)\n    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='k')\n    ax.set_title(title)\n\n# Funzioni di predizione per ogni modalità\npredict_funcs = {\n    \"expert1\": lambda X: np.argmax(get_aligned_proba(expert1, X, 0, 1), axis=1),\n    \"expert2\": lambda X: np.argmax(get_aligned_proba(expert2, X, 1, 2), axis=1),\n    \"expert3\": lambda X: np.argmax(get_aligned_proba(expert3, X, 0, 2), axis=1),\n    \"base\": lambda X: base.predict(X),\n    \"soft_voting\": lambda X: np.argmax(\n        (get_aligned_proba(expert1, X, 0, 1) + get_aligned_proba(expert2, X, 1, 2) + get_aligned_proba(expert3, X, 0, 2)) / 3, axis=1),\n    \"hard_voting\": lambda X: np.array([\n        np.bincount([np.argmax(get_aligned_proba(expert1, X[i:i+1], 0, 1)),np.argmax(get_aligned_proba(expert2, X[i:i+1], 1, 2)),\n                     np.argmax(get_aligned_proba(expert3, X[i:i+1], 0, 2))]).argmax()\n        for i in range(len(X))]),\n    \"gating\": lambda X: np.argmax(\n        get_aligned_proba(expert1, X, 0, 1) * base.predict_proba(X)[:, [0]] +\n        get_aligned_proba(expert2, X, 1, 2) * base.predict_proba(X)[:, [1]] +\n        get_aligned_proba(expert3, X, 0, 2) * base.predict_proba(X)[:, [2]], axis=1)}\n\n# Plot\nfig, axes = plt.subplots(3, 3, figsize=(15, 12))\naxes = axes.ravel()\n\nfor i, mode in enumerate(modes):\n    plot_boundary_from_preds(predict_funcs[mode], X_test_pca, y_test, f\"Boundary: {mode}\", axes[i])\n\n# Rimuovi celle extra se < 9\nfor j in range(len(modes), len(axes)):\n    axes[j].axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T13:40:32.471243Z","iopub.execute_input":"2025-05-15T13:40:32.471564Z","iopub.status.idle":"2025-05-15T13:40:43.401355Z","shell.execute_reply.started":"2025-05-15T13:40:32.471530Z","shell.execute_reply":"2025-05-15T13:40:43.400315Z"}},"outputs":[],"execution_count":null}]}